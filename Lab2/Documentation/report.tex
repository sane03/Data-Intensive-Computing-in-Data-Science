\documentclass[11pt]{IEEEtran}
\onecolumn
\renewcommand\thesection{\arabic{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}}
\renewcommand\thesubsubsection{\thesubsection.\arabic{subsubsection}}
\renewcommand\figurename{Figure.}
\renewcommand{\thetable}{\arabic{table}}

\renewcommand\thesectiondis{\arabic{section}}
\renewcommand\thesubsectiondis{\thesectiondis.\arabic{subsection}}
\renewcommand\thesubsubsectiondis{\thesubsectiondis.\arabic{subsubsection}}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{float}
\usepackage{textcomp}
\usepackage{tikz}
\usepackage[siunitx]{circuitikz}
\usepackage{caption}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{ragged2e}
%\usepackage[]{algorithm2e}
%\usepackage{algpseudocode}
\usepackage{fancyhdr,graphicx,amsmath,amssymb, mathtools, scrextend, titlesec, enumitem}
\usepackage[ruled,linesnumbered]{algorithm2e} 


\begin{document}

	\title{In-place Multithreaded Square Matrix Transpose}
	\author{\IEEEauthorblockN{\textbf{Sanele Ndlovu-716411 \\ Knowledge Dzumba - 813137} \\
			\IEEEauthorblockA{School of Electrical and Information Engineering, University of the Witwatersrand, Johannesburg \\
				ELEN4020A:Data Intensive Computing in Data Science}}}
	\maketitle
	

\section{Introduction}

\noindent
This report presents the results of a programming laboratory work on shared memory programming using OpenMP and Pthread libraries. The purpose of the laboratory work was to explore the functionality of these libraries for simple scientific applications of manipulating very large matrices which are maintained in memory. The matrix operation of focus was in-place transposition, achieved through three different parallel algorithms. The performance of these algorithms was evaluated by observing the time taken to perform the transposition and benchmarking it against the performance of a basic serial algorithm. The sections that follow provide pseucodes for the different algorithms used as well as a brief description of how the algorithms work and a comparative table of performance for the different algorithms implemented using the two different shared programming libraries.


\section{Basic Non-threaded algorithm}

\noindent
\textbf{Algorithm 1} computes the transpose of the input matrix \emph{A} by first creating a secondary matrix \emph{B} and then interchanging the rows and columns of elements of \emph{A} and assigning them to the matrix \emph{B}. Matrix \emph{B} is then returned as the transposed version of \emph{A}. In this algorithm, the original matrix and its transposed version are stored in different memory locations since a new memory allocation had to be made for the returned matrix \emph{B}. \\

\begin{algorithm}[H]
 \KwIn{\emph{n} x \emph{n} matrix \emph{A}}
 \KwOut{\emph{n} x \emph{n} matrix \emph{B}}
 
 \emph{n} = \emph{A.rows} \\
 \emph{Let B be a newly created n x n matrix} \\
 {\For{i = 0 to n - 1}
   {
    \For{j = 0 to n - 1}
    {
     \emph{B[i][j] = A[j][i]}
    }
   }
 }
 \Return{\emph{$B$}}
 \caption{Basic matrix transpose}
\end{algorithm}


\section{OpenMP Naive Threaded Algorithm}

\noindent
\textbf{Algorithm 2} Computes the transpose of a matrix \emph{A} in-place, which means that the same memory locations that stored the input matrix are used to store the transposed matrix without an allocation for a new matrix. The nested \textbf{for} loops in the algorithm iterate through the main diagonal of the matrix, swapping the elements \emph{A[i][j]} with the elements \emph{A[j][i]}. The result of this operation is a transposition of the actual matrix \emph{A}. Using the OpenMP directive \textbf{pragma omp parallel for}, the nested loops are automatically made to be executed by multiple threads if the algorithm is compiled as an OpenMP program. This is because the \textbf{pragma omp parallel for} directive causes multiple threads to be forked at the point of execution, with the multiple threads dividing the workload of the loops.

\begin{algorithm}[H]
 \KwIn{\emph{n} x \emph{n} matrix \emph{A}}
 \KwOut{\emph{n} x \emph{n} matrix \emph{B}}
 \ForPar{ i = 1 to n - 2}
 {
    \ForPar{ j = 1 + 1 to n - 1}
    {
       \emph{swap(A[i][j], A[j][i])}
    }
 }
   
   \Return{$A$}
 \caption{Naive OpenMp matrix transpose}
 
\end{algorithm}



\section{OpenMP Diagonal Threaded Algorithm}
\noindent
\textbf{Algorithm 3} computes the transpose of a matrix \emph{A} in-place. For each diagonal element \emph{A[i][i]}, the algorithm generates 4 threads that swap the elements on the right of the diagonal element with the corresponding elements below the diagonal element. Thread 0 starts by swapping the elements immediately right and below the diagonal element, thread 1 swaps the elements 2 index positions further from the index position, thread 2 swaps elements 3 index positions and thread 3 swaps elements that are 4 index positions from the diagonal index. When thread 0 is done swapping its elements, it continues to swap the elements 5 positions from the index position (that is, you add $numberOfThreads$ to the previous index to get next position swapped by this thread). The other threads follow the same pattern until the row and column for the $i^{th}$ diagonal element are completely interchanged. A consequence of this algorithm is that the size of the array should be divisible by the number of threads used, which in the case of the lab worked well because all the matrix sizes were divisible by 4. The outer \textbf{for} loop ensures that the swapping operation is performed for all diagonal elements(its loop variable determines the diagonal element at which the threads are operating) inner loop performs the actual swapping of elements. \\

\begin{algorithm}[H]
  \KwIn{\emph{n} x \emph{n} matrix \emph{A}}
  \KwOut{\emph{n} x \emph{n} matrix \emph{B}}
  
 \For{ i = 1 to n}
 {
   $start \hookleftarrow threadNumber + i + 1$ \\
   $increament \hookleftarrow numberOfThreads$\\
   
  \ForPar{ j = $start$ to n}
  {
    \emph{swap(A[index][j],A[j][index])} \\
    $j\mathrel{+}= increament$
  }
 }
 \Return{$A$}
 \caption{OpenMP diagonal threaded matrix transpose}
\end{algorithm}


\section{Pthread Diagonal Thread Algorithm}

\noindent
\textbf{Algorithm 4} performs matrix transposition in place, in a similar manner to that of \textbf{Algorithm 3}, the difference being that this algorithm is parallelized using Pthreads instead of OpenMP. While threads are forked through the \textbf{pragma omp parallel} for \textbf{Algorithm 3}, the creation and behaviour of threads is hard coded for this algorithm.
\begin{algorithm}[H]
  \KwIn{\emph{n} x \emph{n} matrix \emph{A}}
  \KwOut{\emph{n} x \emph{n} matrix \emph{B}}
  
 \For{ i = 1 to n}
 {
   $start \hookleftarrow threadNumber + i + 1$ \\
   $increament \hookleftarrow numberOfThreads$\\
   
  \ForPar{ j = $start$ to n}
  {
    \emph{swap(A[index][j],A[j][index])} \\
    $j\mathrel{+}= increament$
  }
 }
 \Return{$A$}
 \caption{Pthread diagonal threaded matrix transpose}
\end{algorithm}

\section{Algorithm Performance}

\noindent
The algorithms mentioned above were compiled on a Lenovo Ideapad 320, core i5 laptop with a 4GB RAM and 4 2.5GHz processor cores. For each algorithm, an $N_0 \times N_1$ square matrix was used as the input to be transposed for varying values of $N_0$ as indicated in TABLE 1. It was required that the input matrix sizes be up to 4096, but to test the scalability of the algorithms for larger inputs, the algorithms were tested with input sizes up to 16384. 

\begin{table}[H] 
\caption{Timing results for parallel libraries}
\centering
\begin{tabular}{|c|c|c|cc|c|}
\hline
$N_0 = N_1$ & Basic & Pthreads & OpenMP&\\
\hline
     &          & Diagonal & Naive    & Diagonal \\
\hline
128  & 0.000271 & 0.024574 & 0.02591  & 0.003667\\
1024 & 0.011464 & 0.060502 & 0.00551  & 0.012012\\
2048 & 0.035667 & 0.119387 & 0.020867 & 0.042493\\
4096 & 0.155627 & 0.271844 & 0.079695 & 0.119827\\
8192 & 0.712869 & 0.816841 & 0.376895 & 0.408006 \\
16384&no terminate& 4.90293& 3.96019  & 2.4639 \\
\hline
\end{tabular}
\end{table}



\end{document}
